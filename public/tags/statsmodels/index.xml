<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>statsmodels on Aaron&#39;s D4ta blog</title>
    <link>https://drwaterx.github.io/til/tags/statsmodels/</link>
    <description>Recent content in statsmodels on Aaron&#39;s D4ta blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 08 Jan 2023 15:18:18 -0500</lastBuildDate><atom:link href="https://drwaterx.github.io/til/tags/statsmodels/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Explainable insights from linear regression of time series</title>
      <link>https://drwaterx.github.io/til/posts/practical_primer_ts/</link>
      <pubDate>Sun, 08 Jan 2023 15:18:18 -0500</pubDate>
      
      <guid>https://drwaterx.github.io/til/posts/practical_primer_ts/</guid>
      <description>This post is under construction, with missing graphs and unbaked LaTax math.
It&amp;rsquo;s a cliche that linear models are &amp;ldquo;explainable.&amp;rdquo; And yet, when we attempt to produce insights that are salient to business, and produce them from the kind of data representing, let&amp;rsquo;s say, tens of thousands of cases, you will quickly find characteristics of the data that appreciably erode the veracity and salience of supposed insights.
This is especially the case when applying canned routines in general, and I&amp;rsquo;ve found, time series &amp;lsquo;forecasting&amp;rsquo; packages in particular.</description>
    </item>
    
  </channel>
</rss>
