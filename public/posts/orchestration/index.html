<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Follow your inner conductor: A comparison of file versus abstract dependency-based orchestration | Aaron&#39;s D4ta blog</title>
<meta name="keywords" content="swe, mlops">
<meta name="description" content="TLDR; GNU Make and Apache Airflow and two DAG orchestration tools. Make gets the job done and is, without a doubt, simpler than Airflow. However, conformance to a purely file dependency-based orchestration (Make) could require refactoring that Airflow would not. It&rsquo;s worth learning how to use Airflow (or another abstract orchestration tool), even if you have no immediate need for most of its features.
Every quarter or so, I deliberately slow down to learn and, if I find it worthwhile, incorporate a new way of working.">
<meta name="author" content="Aaron Slowey">
<link rel="canonical" href="https://drwaterx.github.io/til/posts/orchestration/">
<link crossorigin="anonymous" href="/til/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css" integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/til/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://drwaterx.github.io/til/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://drwaterx.github.io/til/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://drwaterx.github.io/til/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://drwaterx.github.io/til/apple-touch-icon.png">
<link rel="mask-icon" href="https://drwaterx.github.io/til/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<script src="https://cdn.jsdelivr.net/npm/vega@5"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-lite@4.8.1"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-embed@6"></script>
<meta property="og:title" content="Follow your inner conductor: A comparison of file versus abstract dependency-based orchestration" />
<meta property="og:description" content="TLDR; GNU Make and Apache Airflow and two DAG orchestration tools. Make gets the job done and is, without a doubt, simpler than Airflow. However, conformance to a purely file dependency-based orchestration (Make) could require refactoring that Airflow would not. It&rsquo;s worth learning how to use Airflow (or another abstract orchestration tool), even if you have no immediate need for most of its features.
Every quarter or so, I deliberately slow down to learn and, if I find it worthwhile, incorporate a new way of working." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://drwaterx.github.io/til/posts/orchestration/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-02-03T20:56:44-05:00" />
<meta property="article:modified_time" content="2024-02-03T20:56:44-05:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Follow your inner conductor: A comparison of file versus abstract dependency-based orchestration"/>
<meta name="twitter:description" content="TLDR; GNU Make and Apache Airflow and two DAG orchestration tools. Make gets the job done and is, without a doubt, simpler than Airflow. However, conformance to a purely file dependency-based orchestration (Make) could require refactoring that Airflow would not. It&rsquo;s worth learning how to use Airflow (or another abstract orchestration tool), even if you have no immediate need for most of its features.
Every quarter or so, I deliberately slow down to learn and, if I find it worthwhile, incorporate a new way of working."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://drwaterx.github.io/til/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Follow your inner conductor: A comparison of file versus abstract dependency-based orchestration",
      "item": "https://drwaterx.github.io/til/posts/orchestration/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Follow your inner conductor: A comparison of file versus abstract dependency-based orchestration",
  "name": "Follow your inner conductor: A comparison of file versus abstract dependency-based orchestration",
  "description": "TLDR; GNU Make and Apache Airflow and two DAG orchestration tools. Make gets the job done and is, without a doubt, simpler than Airflow. However, conformance to a purely file dependency-based orchestration (Make) could require refactoring that Airflow would not. It\u0026rsquo;s worth learning how to use Airflow (or another abstract orchestration tool), even if you have no immediate need for most of its features.\nEvery quarter or so, I deliberately slow down to learn and, if I find it worthwhile, incorporate a new way of working.",
  "keywords": [
    "swe", "mlops"
  ],
  "articleBody": "TLDR; GNU Make and Apache Airflow and two DAG orchestration tools. Make gets the job done and is, without a doubt, simpler than Airflow. However, conformance to a purely file dependency-based orchestration (Make) could require refactoring that Airflow would not. It’s worth learning how to use Airflow (or another abstract orchestration tool), even if you have no immediate need for most of its features.\nEvery quarter or so, I deliberately slow down to learn and, if I find it worthwhile, incorporate a new way of working. I keep this to perhaps two days to at most two weeks of sunk cost. In other words, I spend 10 to 20 percent of my coding time on strategic, rather than tactical, programming.\nPerhaps it is no surprise that, within three months, the cost is comfortably repaid in productivity, reliability, quality, and/or sanity.\nWhen I evaluate each new tool, I’ve been reminding myself to judge its utility by the degree to which it pushes out the efficiency frontier, rather than merely perturbing the interplay between interface and implementation, modularity and dependency, obscurity and information hiding. These and other design criteria have become quiet, but persistent brain waves vibrating under the tactical notes of the dozens of small tasks executed each day.\nthe cognitive load born of all of these aspects, which inevitably hinders or even prevents the attainment of value.\nConfronted with a sufficiently dependent set of processes involving enough data to forbid unnecessary repetition, I recently refactored a linear pipeline into a directed acyclic graph (DAG) and executed it with both an older and more recent technology: GNU Make and Apache Airflow. Make got the job done and is, without a doubt, simpler than Airflow. However, in my use case, Make required changes to existing code that Airflow did not, and so that cost ultimately led me to equip myself with a more powerful tool for posterity, even though I have no immediate need for most of its features.\nLet’s describe a stylized problem, explain the techniques, and look at some very simple implementations.\nProblem setting We need to produce a half-dozen analyses. The requirements of some analyses imply a more-or-less unique set of data, which further implies a one-to-one ratio of SQL query to analytical code (in Python, in my case) to end product (tables and graphs in a LaTex report). Other analyses, or let’s use the more generic term processes, share data. Some processes consume the same (interim) output of another process. There are clean linear paths from data to interim data to result as well as some tree-like paths. In all cases, queries to obtain starting data are time-consuming, as are some Python processes.\nRepetition would reduce productivity, so as we progress through a sequence of processes, we cache data upon the completion of each stage. Should a downstream error occur or be discovered, don’t rerun completed upstream tasks.\nMy first implementation involved caching files and including if-then logic in a master driver script. If a certain file exists, do not run the methods that would create it; start with the next stage of the pipeline. To limit the program to a subset of tasks, I added a Boolean to the “does the file exist” logic. Recall that some endpoints in my use case were independent of others, but to simplify the interface to the overall project, I would rather not split them off into their own pipelines.\nAs the project grew, repeating that logic started to feel like the wrong approach. What if instead of looking from left to right, we start at the end and work back? In a sense that is what a DAG view is, and is exactly how we compose Makefiles.\nGNU Make A Makefile defines a DAG, formalizing workflow steps in terms of input and output file dependencies, as opposed to abstract (code-configured) dependencies that are definable with other tools like Airflow. make resolves these dependencies and determines which commands need to be run and in what order. It can orchestrate the execution of code in virtually any language and so learning how to use it generalizes and lays a foundation for learning more recent technologies like Drake (Make for data), Pydoit (Python functions, but close to Make), Luigi (more explicit; object-oriented), Airflow and Prefect.\nmake is a command; when executed, it looks for a file called Makefile (no extension) in the current directory. You could instead have a file with a customized name followed by the extension .make, as long as you add -f to the make command; i.e., $ make -f special_file.make. It is more common to use Makefile, so you can just type make.\nHere is the format of a Makefile:\ntarget [target ...]: [prerequisite ...] [recipe] ... ... Think of each line as a recipe, and each sub-line as a list of ingredients (@echo…” “), and the third sub-line as instructions target is the name of a file to create recipe could be the python command to execute a script. Indentations may appear to be 4 spaces, but they need to be a tab. If the name of the result (i.e., the target name) of a task already exists, make will report that the output ‘is up to date.’ It does not check the contents of the target against the intent expressed by the Makefile, just the name and timestamp. List the last task first. The dependencies will be discovered and prior tasks run as Here is another symbolic example with a little more detail:\nstat_summmary.csv: input_data.csv python summarize.py input_data.csv: raw_data.csv python clean.py raw_data.csv: python load_data.py housekeeping: rm -f tempfile.csv\tIf necessary to fulfill the default goal of producing stat_summary.csv, executing make by itself at the CL will run python, which will perform the operations in clean.py, before performing the python summarize.py recipe. If input_data.csv already exists, however, make will just invoke python.summarize.py.\nFor this type of DAG to work, 1 module consumes 1 file and generates 1 file. Constrain function scope accordingly.\nIf we just wanted to perform an upstream task–namely load_data.py–without any downstream task(s), we would execute make input_data.csv.\nDuring an earlier stage, it may be helpful to have the full task pipeline listed in the Makefile, but focus on testing a subset of dependencies. To accomplish that objective, configure the default goal at the top of the Makefile with .DEFAULT_GOAL := input_data.csv, where input_data.csv is the second target in this example. Calling make will then check for raw_data.csv and execute python load_data.py recipe as necessary, ignoring the statistical summary recipe. This ability to easily iterate is one of the advantages of having file dependencies. It is also helpful to have intermediate data stored in files at various stages of transformation to find and resolve errors.\nIf your makefile includes a recipe that does not produce a dependency, you would have to explicitly tell make to run it: make housekeeping in the example above. In this mode, make is shorthand for Unix commands or perhaps a substitute for $ python do_something.py, although the utility of such a recipe is questionable.\nIf a recipe does not produce a file, we say that recipe has a phony target. Since the orchestration capability of make depends on (interim) files, recipes with phony targets (non-target operations) cannot produce dependencies.\nThere is more to GNU make, although the preceding likely covers what’s needed for data processing for statistical modeling. See this terse summary for more.\nCons of Make include lack of scheduling. Also, your program may involve many (phony) targets that, if conformed to this approach, would imply a burdensome number of files (not to mention code to write and read such files).\nHere is a working example you can replicate: Example 1: Compute the average difference of two columns of randomly generated numbers (a) minus an array from a second data file (b). We’ll do 1b with Make and do the simpler 1a in Airflow.\nresult.txt: data1.csv data2.csv python summarize.py data1.csv: python create_data.py data2.csv: python create_data2.py # create_data.py import numpy as np import pandas as pd def create_data(): df = pd.DataFrame({'a': np.random.random(10), 'b': np.random.random(10)}) df.to_csv('data1.csv') if __name__ == \"__main__\": create_data() # create_data2.py import numpy as np import pandas as pd if __name__ == \"__main__\": df = pd.DataFrame({'z': np.random.random(10)}) df.to_csv('data2.csv') # summarize.py import numpy as np import pandas as pd from pathlib import Path def summarize(): df = pd.read_csv('data1.csv') df.loc[:, 'difference'] = np.subtract(df.a, df.b) df2 = pd.read_csv('data2.csv') avg_diff = np.average(df.difference - df2.z) o = Path('result.txt') o.open('w').write(str(avg_diff)) if __name__ == \"__main__\": summarize() In the following sequence, we run Make from an empty state; it shows us what processes ran. Rerun Make and nothing is done other than to notify us that the primary target is “up to date.” Since create_data2 is an un-seeded RNG, rerun the module by itself, and we change a dependency (data2.csv). Make recognizes the change in file metadata and exercises logic to rerun summary.py, but not create_data, since the dependency data.csv already existed unchanged. A minimally viable way to avoid unnecessarily repetitive processes, given that each process consumes and produces one or more files.\n% Make python create_data.py python create_data2.py python summarize.py % Make Make: `result.txt' is up to date. % python create_data2.py % Make python summarize.py Airflow Airflow addresses the same core concern as Make, and many more secondary concerns, accumulated from a variety of use cases, of which building machine learning models is probably only a small fraction. Implications of using Airflow include\nAirflow needs to be installed and setup – this is not difficult as a local installation and setup, but you need to be aware of airflow.cfg and other things. The DAG script, written in Python, includes a lot of (default) arguments Among other preliminary tasks, DAGs need to be deployed (registered?) to a backend database before it can execute the data processing tasks. Executing a DAG (backfill) at the command line logs messages and auto-generated metadata like run_id and run_duration that could aid in auditing, but obtaining signal from noise will require effort. The web browser monitoring dashboard is certainly appealing, such as the Graph view, and another tool to learn Spend time with Airflow, and it’s easy to see why GNU Make could be a better choice for localized processing that is complex enough to warrant a DAG orchestration, but not in need of the execution logging and visualization that Airflow provides.\nOther things to be aware of:\nDocumentation provides illustrative examples in a different way. Sifting through it takes time. Airflow does not explicitly rely on file dependencies. Includes two APIs: Explicitly instantiate ‘operators’, like PythonOperator, BashOperator, EmailOperator, SQLExecuteQueryOperator, OracleOperator TaskFlow – more abstract interface and, therefore, concise Thinking in Airflow jargon An operator defines a unit of work for Airflow to complete. Instantiating operators is the classic approach to defining work in Airflow. For some use cases, it’s better to use the TaskFlow API. The choice of API will determine how you define dependencies.\nLet’s orchestrate Example 1a (average difference of random arrays) with Airflow’s PythonOperator.\n# rng_summarizer.py from datetime import datetime, timedelta from airflow.models.dag import DAG from airflow.operators.python import PythonOperator from create_data import create_data from summarize import summarize with DAG( dag_id=\"rng_summarizer\", default_args={ \"depends_on_past\": False, \"email\": [\"aaron.slowey@gmail.com\"], \"email_on_failure\": False, \"email_on_retry\": False, \"retries\": 1, \"retry_delay\": timedelta(minutes=5), }, description=\"Aaron trying to learn Airflow\", schedule=timedelta(days=1), start_date=datetime(2024, 1, 30), catchup=False, tags=[\"aaron\"], ) as dag: task_id=\"generate_random_numbers\", python_callable=create_data, ) t2 = PythonOperator( task_id=\"take_difference_and_average\", python_callable=summarize, ) t1.set_downstream(t2) Note the Python module name matches the dag_id. I don’t know if this is required, but there is no apparent reason for them to be different, as the module defines the DAG and nothing else. The python_callable can be imported into the DAG definition module, or the functions can reside in that module, and is assigned without ().\nCheck for syntax errors; at the Terminal:\npython ~/airflow/dags/rng_summarizer.py If no exceptions occur, we can rule out some problems, but we do not yet know if Airflow can complete the tasks. At some point, the DAG needs to be parsed; it is unclear if the preceding command parses the DAG or not. Run the following sequence at the Terminal:\n# re-initialize the database tables with newly created/modified DAGs airflow db migrate # Verify by printing the list of active DAGs airflow dags list # Optional: prints the hierarchy of tasks in the \"tutorial\" DAG airflow tasks list name_of_module_defining_dag --tree You can test a task with airflow tasks test; this will ignore dependencies and will not communicate state to the database. Similarly, airflow dags test considers dependencies but also does not communicate state to the database.\nYou can launch various utilities one by one, such as airflow webserver, which will launch a browser utility, or altogether with airflow standalone (at the Terminal). Login to localhost:8080 with username and password provided in stdout upon running standalone. Experience suggests that you do not need to run webserver or standalone to go to the local URL, login, and see a recently completed DAG, provided that you ran airflow db migrate and backfill.\nbackfill is the Airflow command that (conditionally) executes the tasks comprising a DAG (errors are provided for learning):\n$ airflow dags backfill rng_summarizer --start_date 2024-01-30 Dag 'rng_summarizer' could not be found; either it does not exist or it failed to parse. The stdout of a successful run will be verbose with metadata that are logged; potentially useful for tracking your work.\n",
  "wordCount" : "2216",
  "inLanguage": "en",
  "datePublished": "2024-02-03T20:56:44-05:00",
  "dateModified": "2024-02-03T20:56:44-05:00",
  "author":{
    "@type": "Person",
    "name": "Aaron Slowey"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://drwaterx.github.io/til/posts/orchestration/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Aaron's D4ta blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://drwaterx.github.io/til/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://drwaterx.github.io/til/" accesskey="h" title="Aaron&#39;s D4ta blog (Alt + H)">Aaron&#39;s D4ta blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://drwaterx.github.io/til/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://drwaterx.github.io/til/tags" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Follow your inner conductor: A comparison of file versus abstract dependency-based orchestration
    </h1>
    <div class="post-meta">&lt;span title=&#39;2024-02-03 20:56:44 -0500 EST&#39;&gt;February 3, 2024&lt;/span&gt;&amp;nbsp;·&amp;nbsp;Aaron Slowey

</div>
  </header> 
  <div class="post-content"><p><strong>TLDR</strong>; GNU Make and Apache Airflow and two DAG orchestration tools.  <code>Make</code> gets the job done and is, without a doubt, simpler than Airflow.  However, conformance to a purely file dependency-based orchestration (<code>Make</code>) could require refactoring that Airflow would not.  It&rsquo;s worth learning how to use Airflow (or another abstract orchestration tool), even if you have no immediate need for most of its features.</p>
<p>Every quarter or so, I deliberately slow down to learn and, if I find it worthwhile, incorporate a new way of working.  I keep this to perhaps two days to at most two weeks of sunk cost.  In other words, I spend 10 to 20 percent of my coding time on strategic, rather than tactical, programming.</p>
<p>Perhaps it is no surprise that, within three months, the cost is comfortably repaid in productivity, reliability, quality, and/or sanity.</p>
<p>When I evaluate each new tool, I&rsquo;ve been reminding myself to judge its utility by the degree to which it pushes out the efficiency frontier, rather than merely perturbing the interplay between interface and implementation, modularity and dependency, obscurity and information hiding.  These and other design criteria have become quiet, but persistent brain waves vibrating under the tactical notes of the dozens of small tasks executed each day.</p>
<p>the cognitive load born of all of these aspects, which inevitably hinders or even prevents the attainment of value.</p>
<p>Confronted with a sufficiently dependent set of processes involving enough data to forbid unnecessary repetition, I recently refactored a linear pipeline into a directed acyclic graph (DAG) and executed it with both an older and more recent technology: GNU Make and Apache Airflow.  Make got the job done and is, without a doubt, simpler than Airflow.  However, in my use case, Make required changes to existing code that Airflow did not, and so that cost ultimately led me to equip myself with a more powerful tool for posterity, even though I have no immediate need for most of its features.</p>
<p>Let&rsquo;s describe a stylized problem, explain the techniques, and look at some very simple implementations.</p>
<h2 id="problem-setting">Problem setting<a hidden class="anchor" aria-hidden="true" href="#problem-setting">#</a></h2>
<p>We need to produce a half-dozen analyses.  The requirements of some analyses imply a more-or-less unique set of data, which further implies a one-to-one ratio of SQL query to analytical code (in Python, in my case) to end product (tables and graphs in a LaTex report).  Other analyses, or let&rsquo;s use the more generic term processes, share data.  Some processes consume the same (interim) output of another process.  There are clean linear paths from data to interim data to result as well as some tree-like paths.  In all cases, queries to obtain starting data are time-consuming, as are some Python processes.</p>
<p>Repetition would reduce productivity, so as we progress through a sequence of processes, we cache data upon the completion of each stage. Should a downstream error occur or be discovered, don&rsquo;t rerun completed upstream tasks.</p>
<p>My first implementation involved caching files and including if-then logic in a master driver script.  If a certain file exists, do not run the methods that would create it; start with the next stage of the pipeline.  To limit the program to a subset of tasks, I added a Boolean to the &ldquo;does the file exist&rdquo; logic.  Recall that some endpoints in my use case were independent of others, but to simplify the interface to the overall project, I would rather not split them off into their own pipelines.</p>
<p>As the project grew, repeating that logic started to feel like the wrong approach.  What if instead of looking from left to right, we start at the end and work back?  In a sense that is what a DAG view is, and is exactly how we compose Makefiles.</p>
<h2 id="gnu-make">GNU Make<a hidden class="anchor" aria-hidden="true" href="#gnu-make">#</a></h2>
<p>A Makefile defines a DAG, formalizing workflow steps in terms of input and output <em>file</em> dependencies, as opposed to <em>abstract</em> (code-configured) dependencies that are definable with other tools like Airflow. <code>make</code> resolves these dependencies and determines which commands need to be run and in what order.  It can orchestrate the execution of code in virtually any language and so learning how to use it generalizes and lays a foundation for learning more recent technologies like Drake (<code>Make</code> for data), <code>Pydoit</code> (Python functions, but close to <code>Make</code>), <code>Luigi</code> (more explicit; object-oriented), Airflow and Prefect.</p>
<p><code>make</code> is a command; when executed, it looks for a file called <code>Makefile</code> (no extension) in the current directory.  You could instead have a file with a customized name followed by the extension <code>.make</code>, as long as you add <code>-f </code> to the <code>make</code> command; i.e., <code>$ make -f special_file.make</code>. It is more common to use <code>Makefile</code>, so you can just type <code>make</code>.</p>
<p>Here is the format of a Makefile:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-make" data-lang="make"><span style="display:flex;"><span><span style="color:#a6e22e">target [target ...]</span><span style="color:#f92672">:</span> [prerequisite ...]
</span></span><span style="display:flex;"><span>	<span style="color:#f92672">[</span>recipe<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>	...
</span></span><span style="display:flex;"><span>	...
</span></span></code></pre></div><ul>
<li>Think of each line as a recipe, and each sub-line as a list of ingredients (@echo…” “), and the third sub-line as instructions</li>
<li><code>target</code> is the name of a file to create</li>
<li><code>recipe</code> could be the <code>python</code> command to execute a script.</li>
<li>Indentations may appear to be 4 spaces, but they need to be a <em>tab</em>.</li>
<li>If the <em>name</em> of the result (i.e., the <em>target</em> name) of a task already exists, <code>make</code> will report that the output &lsquo;is up to date.&rsquo;  It does not check the contents of the target against the intent expressed by the Makefile, just the name and timestamp.</li>
<li>List the last task first.  The dependencies will be discovered and prior tasks run as</li>
</ul>
<p>Here is another symbolic example with a little more detail:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-make" data-lang="make"><span style="display:flex;"><span><span style="color:#a6e22e">stat_summmary.csv</span><span style="color:#f92672">:</span> input_data.csv
</span></span><span style="display:flex;"><span>	python summarize.py
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">input_data.csv</span><span style="color:#f92672">:</span> raw_data.csv
</span></span><span style="display:flex;"><span>	python clean.py
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">raw_data.csv</span><span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>	python load_data.py
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">housekeeping</span><span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>	rm -f tempfile.csv		
</span></span></code></pre></div><p>If necessary to fulfill the <em>default goal</em> of producing <code>stat_summary.csv</code>, executing <code>make</code> by itself at the CL will run <code>python</code>, which will perform the operations in <code>clean.py</code>, before performing the <code>python summarize.py</code> recipe.  If <code>input_data.csv</code> already exists, however, <code>make</code> will just invoke <code>python.summarize.py</code>.</p>
<p>For this type of DAG to work, 1 module consumes 1 file and generates 1 file.  Constrain function scope accordingly.</p>
<p>If we just wanted to perform an upstream task&ndash;namely <code>load_data.py</code>&ndash;without any downstream task(s), we would execute <code>make input_data.csv</code>.</p>
<p>During an earlier stage, it may be helpful to have the full task pipeline listed in the <code>Makefile</code>, but focus on testing a subset of dependencies.  To accomplish that objective, configure the default goal at the top of the <code>Makefile</code> with <code>.DEFAULT_GOAL := input_data.csv</code>, where <code>input_data.csv</code> is the second target in this example.  Calling <code>make</code> will then check for <code>raw_data.csv</code> and execute <code>python load_data.py</code> recipe as necessary, ignoring the statistical summary recipe.  This ability to easily iterate is one of the advantages of having file dependencies.  It is also helpful to have intermediate data stored in files at various stages of transformation to find and resolve errors.</p>
<p>If your <code>makefile</code> includes a recipe that does <em>not</em> produce a dependency, you would have to explicitly tell <code>make</code> to run it: <code>make housekeeping</code> in the example above.  In this mode, <code>make</code> is shorthand for Unix commands or perhaps a substitute for <code>$ python do_something.py</code>, although the utility of such a recipe is questionable.</p>
<p>If a recipe does not produce a file, we say that recipe has a <em>phony</em> target.  Since the orchestration capability of <code>make</code> depends on (interim) files, recipes with phony targets (non-target operations) cannot produce dependencies.</p>
<p>There is more to GNU <code>make</code>, although the preceding likely covers what&rsquo;s needed for data processing for statistical modeling.  See this <a href="https://alexharv074.github.io/2019/12/26/gnu-make-for-devops-engineers.html">terse summary</a> for more.</p>
<p>Cons of Make include lack of scheduling.  Also, your program may involve many (phony) targets that, if conformed to this approach, would imply a burdensome number of files (not to mention code to write and read such files).</p>
<p>Here is a working example you can replicate: Example 1: Compute the average difference of two columns of randomly generated numbers (a) minus an array from a second data file (b).  We&rsquo;ll do 1b with <code>Make</code> and do the simpler 1a in Airflow.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Makefile" data-lang="Makefile"><span style="display:flex;"><span><span style="color:#a6e22e">result.txt</span><span style="color:#f92672">:</span> data1.csv data2.csv  
</span></span><span style="display:flex;"><span>    python summarize.py  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">data1.csv</span><span style="color:#f92672">:</span>  
</span></span><span style="display:flex;"><span>    python create_data.py  
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">data2.csv</span><span style="color:#f92672">:</span>  
</span></span><span style="display:flex;"><span>    python create_data2.py
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># create_data.py</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_data</span>():  
</span></span><span style="display:flex;"><span>    df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame({<span style="color:#e6db74">&#39;a&#39;</span>: np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>random(<span style="color:#ae81ff">10</span>), <span style="color:#e6db74">&#39;b&#39;</span>: np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>random(<span style="color:#ae81ff">10</span>)})  
</span></span><span style="display:flex;"><span>    df<span style="color:#f92672">.</span>to_csv(<span style="color:#e6db74">&#39;data1.csv&#39;</span>)  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:  
</span></span><span style="display:flex;"><span>    create_data()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># create_data2.py</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:  
</span></span><span style="display:flex;"><span>    df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame({<span style="color:#e6db74">&#39;z&#39;</span>: np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>random(<span style="color:#ae81ff">10</span>)})  
</span></span><span style="display:flex;"><span>    df<span style="color:#f92672">.</span>to_csv(<span style="color:#e6db74">&#39;data2.csv&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># summarize.py</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pathlib <span style="color:#f92672">import</span> Path
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">summarize</span>():  
</span></span><span style="display:flex;"><span>    df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#39;data1.csv&#39;</span>)  
</span></span><span style="display:flex;"><span>    df<span style="color:#f92672">.</span>loc[:, <span style="color:#e6db74">&#39;difference&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>subtract(df<span style="color:#f92672">.</span>a, df<span style="color:#f92672">.</span>b)  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>    df2 <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#39;data2.csv&#39;</span>)  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>    avg_diff <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>average(df<span style="color:#f92672">.</span>difference <span style="color:#f92672">-</span> df2<span style="color:#f92672">.</span>z)  
</span></span><span style="display:flex;"><span>    o <span style="color:#f92672">=</span> Path(<span style="color:#e6db74">&#39;result.txt&#39;</span>)  
</span></span><span style="display:flex;"><span>    o<span style="color:#f92672">.</span>open(<span style="color:#e6db74">&#39;w&#39;</span>)<span style="color:#f92672">.</span>write(str(avg_diff))  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:  
</span></span><span style="display:flex;"><span>    summarize()
</span></span></code></pre></div><p>In the following sequence, we run <code>Make</code> from an empty state; it shows us what processes ran.  Rerun Make and nothing is done other than to notify us that the primary target is &ldquo;up to date.&rdquo;  Since <code>create_data2</code> is an un-seeded RNG, rerun the module by itself, and we change a dependency (<code>data2.csv</code>).  <code>Make</code> recognizes the change in file metadata and exercises logic to rerun <code>summary.py</code>, but not <code>create_data</code>, since the dependency <code>data.csv</code> already existed unchanged.  A minimally viable way to avoid unnecessarily repetitive processes, given that each process consumes and produces one or more files.</p>
<pre tabindex="0"><code>% Make
python create_data.py
python create_data2.py
python summarize.py
% Make
Make: `result.txt&#39; is up to date.
% python create_data2.py
% Make                  
python summarize.py
</code></pre><h2 id="airflow">Airflow<a hidden class="anchor" aria-hidden="true" href="#airflow">#</a></h2>
<p>Airflow addresses the same core concern as <code>Make</code>, and many more secondary concerns, accumulated from a variety of use cases, of which building machine learning models is probably only a small fraction. Implications of using Airflow include</p>
<ul>
<li>Airflow needs to be installed and setup &ndash; this is not difficult as a local installation and setup, but you need to be aware of <code>airflow.cfg</code> and other things.</li>
<li>The DAG script, written in Python, includes a lot of (default) arguments</li>
<li>Among other preliminary tasks, DAGs need to be deployed (registered?) to a backend database before it can execute the data processing tasks.</li>
<li>Executing a DAG (<code>backfill</code>) at the command line logs messages and auto-generated metadata like <code>run_id</code> and <code>run_duration</code> that could aid in auditing, but obtaining signal from noise will require effort.</li>
<li>The web browser monitoring dashboard is certainly appealing, such as the Graph view, and another tool to learn</li>
</ul>
<p>Spend time with Airflow, and it&rsquo;s easy to see why GNU Make could be a better choice for localized processing that is complex enough to warrant a DAG orchestration, but not in need of the execution logging and visualization that Airflow provides.</p>
<p>Other things to be aware of:</p>
<ul>
<li>Documentation provides illustrative examples in a different way.  Sifting through it  takes time.</li>
<li>Airflow does not explicitly rely on file dependencies.</li>
<li>Includes two APIs:
<ol>
<li>Explicitly instantiate &lsquo;operators&rsquo;, like <code>PythonOperator</code>, <code>BashOperator</code>, <code>EmailOperator</code>, <code>SQLExecuteQueryOperator</code>, <code>OracleOperator</code></li>
<li><code>TaskFlow</code> &ndash; more abstract interface and, therefore, concise</li>
</ol>
</li>
</ul>
<h3 id="thinking-in-airflow-jargon">Thinking in Airflow jargon<a hidden class="anchor" aria-hidden="true" href="#thinking-in-airflow-jargon">#</a></h3>
<p>An operator defines a unit of work for Airflow to complete. Instantiating operators is the classic approach to defining work in Airflow. For some use cases, it’s better to use the <a href="https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html">TaskFlow API</a>.  The choice of API will determine how you define dependencies.</p>
<p>Let&rsquo;s orchestrate Example 1a (average difference of random arrays) with Airflow&rsquo;s <code>PythonOperator</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># rng_summarizer.py</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> datetime <span style="color:#f92672">import</span> datetime, timedelta  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> airflow.models.dag <span style="color:#f92672">import</span> DAG  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> airflow.operators.python <span style="color:#f92672">import</span> PythonOperator  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> create_data <span style="color:#f92672">import</span> create_data  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> summarize <span style="color:#f92672">import</span> summarize
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> DAG(  
</span></span><span style="display:flex;"><span>    dag_id<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;rng_summarizer&#34;</span>,  
</span></span><span style="display:flex;"><span>	default_args<span style="color:#f92672">=</span>{  
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;depends_on_past&#34;</span>: <span style="color:#66d9ef">False</span>,  
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;email&#34;</span>: [<span style="color:#e6db74">&#34;aaron.slowey@gmail.com&#34;</span>],  
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;email_on_failure&#34;</span>: <span style="color:#66d9ef">False</span>,  
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;email_on_retry&#34;</span>: <span style="color:#66d9ef">False</span>,  
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;retries&#34;</span>: <span style="color:#ae81ff">1</span>,  
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;retry_delay&#34;</span>: timedelta(minutes<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>),  
</span></span><span style="display:flex;"><span>    },  
</span></span><span style="display:flex;"><span>    description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Aaron trying to learn Airflow&#34;</span>,  
</span></span><span style="display:flex;"><span>    schedule<span style="color:#f92672">=</span>timedelta(days<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),  
</span></span><span style="display:flex;"><span>    start_date<span style="color:#f92672">=</span>datetime(<span style="color:#ae81ff">2024</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">30</span>),  
</span></span><span style="display:flex;"><span>    catchup<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>,  
</span></span><span style="display:flex;"><span>    tags<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;aaron&#34;</span>],  
</span></span><span style="display:flex;"><span>) <span style="color:#66d9ef">as</span> dag:  
</span></span><span style="display:flex;"><span>        task_id<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;generate_random_numbers&#34;</span>,  
</span></span><span style="display:flex;"><span>        python_callable<span style="color:#f92672">=</span>create_data,  
</span></span><span style="display:flex;"><span>    )  
</span></span><span style="display:flex;"><span>    t2 <span style="color:#f92672">=</span> PythonOperator(  
</span></span><span style="display:flex;"><span>        task_id<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;take_difference_and_average&#34;</span>,  
</span></span><span style="display:flex;"><span>        python_callable<span style="color:#f92672">=</span>summarize,  
</span></span><span style="display:flex;"><span>    )  
</span></span><span style="display:flex;"><span>    t1<span style="color:#f92672">.</span>set_downstream(t2)
</span></span></code></pre></div><p>Note the Python module name matches the <code>dag_id</code>.  I don&rsquo;t know if this is required, but there is no apparent reason for them to be different, as the module defines the DAG and nothing else.  The <code>python_callable</code> can be imported into the DAG definition module, or the functions can reside in that module, and is assigned without <code>()</code>.</p>
<p>Check for syntax errors; at the Terminal:</p>
<pre tabindex="0"><code class="language-terminal" data-lang="terminal">python ~/airflow/dags/rng_summarizer.py
</code></pre><p>If no exceptions occur, we can rule out some problems, but we do not yet know if Airflow can complete the tasks.  At some point, the DAG needs to be parsed; it is unclear if the preceding command parses the DAG or not.  Run the following sequence at the Terminal:</p>
<pre tabindex="0"><code># re-initialize the database tables with newly created/modified DAGs
airflow db migrate

# Verify by printing the list of active DAGs
airflow dags list

# Optional: prints the hierarchy of tasks in the &#34;tutorial&#34; DAG
airflow tasks list name_of_module_defining_dag --tree
</code></pre><p>You can test a task with <code>airflow tasks test</code>; this will ignore dependencies and will not communicate state to the database.  Similarly, <code>airflow dags test</code> considers dependencies but also does not communicate state to the database.</p>
<p>You can launch various utilities one by one, such as <code>airflow webserver</code>, which will launch a browser utility, or altogether with <code>airflow standalone</code> (at the Terminal). Login to <code>localhost:8080</code> with username and password provided in stdout upon running <code>standalone</code>.  Experience suggests that you do <em>not</em> need to run <code>webserver</code> or <code>standalone</code> to go to the local URL, login, and see a recently completed DAG, provided that you ran <code>airflow db migrate</code> and <code>backfill</code>.</p>
<p><code>backfill</code> is the Airflow command that (conditionally) executes the tasks comprising a DAG (errors are provided for learning):</p>
<pre tabindex="0"><code class="language-terminal" data-lang="terminal">$ airflow dags backfill rng_summarizer --start_date 2024-01-30
Dag &#39;rng_summarizer&#39; could not be found; either it does not exist or it failed to parse.
</code></pre><p>The stdout of a successful run will be verbose with metadata that are logged; potentially useful for tracking your work.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://drwaterx.github.io/til/tags/swe/">swe</a></li>
      <li><a href="https://drwaterx.github.io/til/tags/mlops/">mlops</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://drwaterx.github.io/til/">Aaron&#39;s D4ta blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
